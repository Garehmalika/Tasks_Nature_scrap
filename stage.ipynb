{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\"Write a Python script that scrapes product details, raw materials details (from the provided Excel file), and brand details from the website https://natrue.org/our-standard/natrue-certified-world/?database[tab]=products. Extract data for each product, including product name, description, certification status, and other available details. Also, extract raw materials and brand information. Store all details in a structured JSON format and then export this data into Google Sheets. Ensure the script handles pagination on the website.\"\n"
      ],
      "metadata": {
        "id": "SyJIiPOYbsL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztJFy44BasEQ",
        "outputId": "c0a9dda0-2030-49af-b4a2-01e48b42c102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.1.4)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.1)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.9.1->oauth2client) (3.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas gspread oauth2client\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Fonction pour r√©cup√©rer les d√©tails des produits\n",
        "def get_product_details():\n",
        "    url = \"https://natrue.org/our-standard/natrue-certified-world/?database[tab]=products\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extraire les informations des produits, ici un exemple\n",
        "    products = []\n",
        "    for product in soup.find_all('div', class_='product-class'):  # Exemple de classe √† ajuster\n",
        "        name = product.find('h3').text\n",
        "        description = product.find('p', class_='product-description').text\n",
        "        certification = product.find('span', class_='certification-status').text\n",
        "        products.append({\"name\": name, \"description\": description, \"certification\": certification})\n",
        "\n",
        "    return products\n",
        "\n",
        "# Fonction pour envoyer les donn√©es vers Google Sheets\n",
        "def send_to_google_sheets(data, sheet_name=\"ProductData\"):\n",
        "    # Cr√©er des credentials pour Google Sheets API\n",
        "    scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/drive']\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name('/content/Projet_stage.json', scope)\n",
        "    client = gspread.authorize(creds)\n",
        "\n",
        "    # Ouvrir la feuille de calcul\n",
        "    # Ouvrir la feuille de calcul\n",
        "    sheet = client.open('stage').sheet1  # Remarquez les guillemets autour de 'stage'\n",
        "\n",
        "    # Convertir en dataframe et envoyer\n",
        "    df = pd.DataFrame(data)\n",
        "    for i, row in df.iterrows():\n",
        "        sheet.append_row(row.values.tolist())\n",
        "\n",
        "# R√©cup√©rer les d√©tails des produits et envoyer √† Google Sheets\n",
        "products = get_product_details()\n",
        "send_to_google_sheets(products)\n",
        "\n",
        "# Sauvegarder en JSON\n",
        "with open('/content/sample_data/data.json', 'w') as json_file:\n",
        "    json.dump(products, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "fPMItdDwazPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temps estim√© : 6‚Äì8 heures (scraping + tests + int√©gration)."
      ],
      "metadata": {
        "id": "ZFRiAgAzaNmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T√¢che 2 : G√©n√©rer un prompt GPT pour extraire les mati√®res premi√®res de l'INCI et les structurer"
      ],
      "metadata": {
        "id": "yKoqM4NpXQGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python script that extracts individual raw materials from an INCI list and structures them similar to the material page at https://www.commonshare.com/materials/alpaca. The script should extract relevant details such as material name, description, properties, standards, and countries associated with each material. Ensure that the standards and countries are correctly matched with the materials. The final structure should be a well-organized data format, such as JSON, with each material having its own page for documentation.\n"
      ],
      "metadata": {
        "id": "mK6CBuEPXVuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 google-auth google-auth-oauthlib google-auth-httplib2 gspread\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxQj5StYx8l",
        "outputId": "86d68ef6-d5a0-44a3-b23f-a1ec931a4c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Act as a data engineer. Write a Python script to parse INCI strings (ex: 'Aqua, Glycerin, Cocos Nucifera Oil') into individual raw materials. For each material, create a Google Doc mimicking https://www.commonshare.com/materials/alpaca with:\n",
        "\n",
        "1. Standardized name (ISO 16128 ou INCI).\n",
        "2. CAS Number (via PubChem API).\n",
        "3. Sourcing Countries (g√©olocalisation via Wikidata).\n",
        "4. Certifications (COSMOS, Ecocert, etc.).\n",
        "5. Structure JSON : {name: 'Cocos Nucifera Oil', type: 'Plant', origin: ['Philippines', 'Brazil'], certifications: ['COSMOS']}.\n",
        "\n",
        "Validate standards and countries with official databases. Save outputs to Google Drive.\""
      ],
      "metadata": {
        "id": "tCjnstHdN22A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests gspread google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client pywikibot pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln6JTOH6O1ow",
        "outputId": "46e3cc02-ed7e-4fdb-d255-390f420469f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.1.4)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.160.0)\n",
            "Collecting pywikibot\n",
            "  Downloading pywikibot-10.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Collecting mwparserfromhell>=0.5.2 (from pywikibot)\n",
            "  Downloading mwparserfromhell-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pywikibot) (24.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.69.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Downloading pywikibot-10.0.0-py3-none-any.whl (718 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m718.5/718.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mwparserfromhell-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mwparserfromhell, pywikibot\n",
            "Successfully installed mwparserfromhell-0.6.6 pywikibot-10.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "def parse_inci(inci_string):\n",
        "    materials = inci_string.split(', ')\n",
        "    for material in materials:\n",
        "        # Recherche CAS via PubChem\n",
        "        cas = requests.get(f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{material}/property/InChIKey/TXT\")\n",
        "        # G√©olocalisation via Wikidata...\n",
        "        # Cr√©ation Google Doc\n",
        "        doc_body = {\n",
        "            'title': material,\n",
        "            'body': {\n",
        "                'content': [{\n",
        "                    'paragraph': {\n",
        "                        'elements': [{\n",
        "                            'textRun': {'content': f\"CAS: {cas}\\nOrigin: {countries}\"}\n",
        "                        }]\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        }\n",
        "        drive_service.documents().create(body=doc_body).execute()"
      ],
      "metadata": {
        "id": "rhM7O5U4NJr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_inci_string(inc_str):\n",
        "    materials = [material.strip() for material in inc_str.split(',')]\n",
        "    return materials\n"
      ],
      "metadata": {
        "id": "JeKAMRxLPMnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_cas_number(material):\n",
        "    url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{material}/property/CAS/JSON'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        try:\n",
        "            return data['PropertyTable']['Properties'][0]['CAS']\n",
        "        except (KeyError, IndexError):\n",
        "            return None\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "ZHiVF8pUPQjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYWIKIBOT_NO_USER_CONFIG\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "vpDjJPNpQTb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.http import MediaFileUpload\n"
      ],
      "metadata": {
        "id": "qnPBCEktU1qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "good :16HoOyG6PzmZcf4a8OoWXvhVjsp62n9Tr"
      ],
      "metadata": {
        "id": "vAT_L1sjZxs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from google.oauth2.service_account import Credentials\n",
        "import time\n",
        "\n",
        "# 1. Configuration des scopes et credentials\n",
        "SCOPES = [\n",
        "    \"https://www.googleapis.com/auth/drive\",\n",
        "    \"https://www.googleapis.com/auth/documents\"\n",
        "]\n",
        "\n",
        "def get_google_credentials(credential_path):\n",
        "    return Credentials.from_service_account_file(credential_path, scopes=SCOPES)\n",
        "\n",
        "# 2. Parsing de la cha√Æne INCI\n",
        "def parse_inci_string(inci_str):\n",
        "    return [material.strip() for material in inci_str.split(',')]\n",
        "\n",
        "# 3. R√©cup√©ration du num√©ro CAS depuis PubChem (avec gestion d'erreur)\n",
        "def get_cas_number(material):\n",
        "    url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{material}/property/CAS/JSON'\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['PropertyTable']['Properties'][0]['CAS']\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur CAS pour {material}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# 4. G√©olocalisation depuis Wikidata (requ√™te SPARQL am√©lior√©e)\n",
        "def get_geolocation_from_wikidata(material):\n",
        "    query = f\"\"\"\n",
        "    SELECT ?countryLabel WHERE {{\n",
        "        ?item rdfs:label \"{material}\"@en.\n",
        "        ?item wdt:P17 ?country.\n",
        "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            \"https://query.wikidata.org/sparql\",\n",
        "            params={'query': query, 'format': 'json'},\n",
        "            headers={'User-Agent': 'Mozilla/5.0'},\n",
        "            timeout=15\n",
        "        )\n",
        "        return list(set([result['countryLabel']['value'] for result in response.json()['results']['bindings']]))\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur Wikidata pour {material}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# 5. Cr√©ation du document Google dans votre dossier personnel\n",
        "def create_google_doc(drive_service, docs_service, material_data, folder_id):\n",
        "    try:\n",
        "        # Cr√©ation du document dans le dossier sp√©cifi√©\n",
        "        file_metadata = {\n",
        "            'name': f\"Fiche - {material_data['name']}\",\n",
        "            'parents': [folder_id],\n",
        "            'mimeType': 'application/vnd.google-apps.document'\n",
        "        }\n",
        "        doc = drive_service.files().create(body=file_metadata).execute()\n",
        "        doc_id = doc['id']\n",
        "\n",
        "        # Ajout du contenu\n",
        "        requests = [{\n",
        "            'insertText': {\n",
        "                'location': {'index': 1},\n",
        "                'text': (\n",
        "                    f\"Nom: {material_data['name']}\\n\"\n",
        "                    f\"CAS: {material_data['cas']}\\n\"\n",
        "                    f\"Origines: {', '.join(material_data['origins'])}\\n\"\n",
        "                    f\"Certifications: {', '.join(material_data['certifications'])}\\n\"\n",
        "                )\n",
        "            }\n",
        "        }]\n",
        "        docs_service.documents().batchUpdate(\n",
        "            documentId=doc_id,\n",
        "            body={'requests': requests}\n",
        "        ).execute()\n",
        "\n",
        "        print(f\"‚úÖ Document cr√©√©: https://docs.google.com/document/d/{doc_id}\")\n",
        "        return doc_id\n",
        "    except HttpError as error:\n",
        "        print(f\"‚ùå Erreur Google API: {error}\")\n",
        "        return None\n",
        "\n",
        "# 6. Processus principal\n",
        "def process_inci(inci_str, credentials_path, folder_id):\n",
        "    credentials = get_google_credentials(credentials_path)\n",
        "    drive_service = build('drive', 'v3', credentials=credentials)\n",
        "    docs_service = build('docs', 'v1', credentials=credentials)\n",
        "\n",
        "    for material in parse_inci_string(inci_str):\n",
        "        print(f\"\\nüîç Traitement de: {material}\")\n",
        "\n",
        "        doc_id = create_google_doc(\n",
        "            drive_service,\n",
        "            docs_service,\n",
        "            {\n",
        "                'name': material,\n",
        "                'cas': get_cas_number(material) or \"Non trouv√©\",\n",
        "                'origins': get_geolocation_from_wikidata(material),\n",
        "                'certifications': ['COSMOS']  # √Ä adapter avec votre logique\n",
        "            },\n",
        "            folder_id\n",
        "        )\n",
        "        time.sleep(1)  # Respect des quotas d'API\n",
        "\n",
        "# 7. Ex√©cution\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration requise\n",
        "    INCI_EXAMPLE = \"Aqua, Glycerin, Cocos Nucifera Oil\"\n",
        "    CREDENTIALS_PATH = \"/content/projetstage1.json\"  # √Ä modifier\n",
        "    FOLDER_ID = \"16HoOyG6PzmZcf4a8OoWXvhVjsp62n9Tr\"  # √Ä modifier\n",
        "\n",
        "    process_inci(INCI_EXAMPLE, CREDENTIALS_PATH, FOLDER_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BIj0RJjUlS7",
        "outputId": "be545c54-9324-4331-9666-0bf867dc69f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Traitement de: Aqua\n",
            "‚úÖ Document cr√©√©: https://docs.google.com/document/d/1JzOzLtS7S4ooaKqsXgy1suWcBBk_OzaaIe4LwJttjeo\n",
            "\n",
            "üîç Traitement de: Glycerin\n",
            "‚úÖ Document cr√©√©: https://docs.google.com/document/d/14bz5jkp-hhozQ8ILB1UgPD-ujiIJ5nz_n4nMJSqBTAA\n",
            "\n",
            "üîç Traitement de: Cocos Nucifera Oil\n",
            "‚úÖ Document cr√©√©: https://docs.google.com/document/d/1Z7cJvSlgMKEnVxTDKnqrW1lQfPi90BdjFqyPoOyohng\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3 (Extra Credit) : Trouver les Contacts Cl√©s"
      ],
      "metadata": {
        "id": "fJQk-Vdte7AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outils Low-Code :\n",
        "\n",
        "\n",
        "\n",
        "1.   Apollo.io ou Hunter.io : Pour trouver des emails et postes.\n",
        "2.   LinkedIn Sales Navigator : Pour identifier les profils.\n",
        "3.   Zapier : Automatiser la sauvegarde dans Google Sheets.\n",
        "4.   Phantombuster : Scraper LinkedIn automatiquement.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7KQsuaVKfHv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.oauth2.service_account import Credentials\n",
        "import gspread\n",
        "import re\n",
        "\n",
        "# 1. Connexion √† Google Sheets (version corrig√©e)\n",
        "def connect_google_sheets(credentials_path, sheet_id):\n",
        "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
        "\n",
        "    try:\n",
        "        creds = Credentials.from_service_account_file(credentials_path, scopes=SCOPES)\n",
        "        client = gspread.authorize(creds)\n",
        "        return client.open_by_key(sheet_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de connexion: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# 2. Extraction du nom de marque (version am√©lior√©e)\n",
        "def extract_brand(name):\n",
        "    try:\n",
        "        # Cas 1 : S√©parateur \"T/A\"\n",
        "        if \" T/A \" in name:\n",
        "            return name.split(\" T/A \")[-1].split(\"(\")[0].strip()\n",
        "\n",
        "        # Cas 2 : Marque entre crochets ou avec ‚Ñ¢\n",
        "        brand_match = re.search(r'(\\[.*?\\]‚Ñ¢)|\\((.*?)\\)', name)\n",
        "        if brand_match:\n",
        "            return brand_match.group(1) or brand_match.group(2)\n",
        "\n",
        "        # Cas 3 : D√©tection automatique du fabricant\n",
        "        patterns = [\n",
        "            r'(.*?)\\s+(GmbH|SAS|AG|Ltd|INC|LLC)',\n",
        "            r'(.*?)\\s+\\d',\n",
        "            r'^(.*?)\\s+-\\s+'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.match(pattern, name)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "\n",
        "        return name.split()[0]\n",
        "    except:\n",
        "        return \"N/A\"\n",
        "\n",
        "# 3. Nettoyage des noms (version s√©curis√©e)\n",
        "def clean_company_names(df):\n",
        "    try:\n",
        "        df = df.copy()\n",
        "        df['cleaned_name'] = df['Brand Name'].str.strip().str.upper()\n",
        "\n",
        "        # Suppression des entit√©s l√©gales et caract√®res sp√©ciaux\n",
        "        df['cleaned_name'] = df['cleaned_name'].str.replace(\n",
        "            r'\\b(INC|LLC|LTD|SA|SAS|GMBH|PTY|CORP|PLC)\\b[,.]?$',\n",
        "            '',\n",
        "            regex=True\n",
        "        ).str.strip()\n",
        "\n",
        "        df['cleaned_name'] = df['cleaned_name'].apply(\n",
        "            lambda x: re.sub(r'[^a-zA-Z0-9√Ä-√ø& ]', '', x) if isinstance(x, str) else x\n",
        "        )\n",
        "\n",
        "        return df.drop_duplicates('cleaned_name').dropna(subset=['cleaned_name'])\n",
        "    except KeyError:\n",
        "        print(\"Colonne 'Brand Name' manquante!\")\n",
        "        return df\n",
        "\n",
        "# 4. Workflow principal (version corrig√©e)\n",
        "def main():\n",
        "    # Configuration\n",
        "    CREDS_PATH = '/content/projetstage1.json'\n",
        "    SHEET_ID = '1gPypXrLKXphNi03Y9cl9njUJ7XEPFU6jaIETt7O4NdA'\n",
        "    SHEET_NAME = 'Worksheet'\n",
        "\n",
        "    try:\n",
        "        # Connexion\n",
        "        sheet = connect_google_sheets(CREDS_PATH, SHEET_ID)\n",
        "        if not sheet:\n",
        "            return\n",
        "\n",
        "        worksheet = sheet.worksheet(SHEET_NAME)\n",
        "        data = worksheet.get_all_values()\n",
        "\n",
        "        # Cr√©ation du DataFrame\n",
        "        df = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "        # Extraction des marques\n",
        "        df[\"Brand Name\"] = df[\"Name\"].apply(extract_brand)\n",
        "\n",
        "        # Nettoyage\n",
        "        cleaned_df = clean_company_names(df)\n",
        "\n",
        "        # Export Apollo\n",
        "        apollo_template = pd.DataFrame({\n",
        "            'Company Name': cleaned_df['cleaned_name'],\n",
        "            'Target Job Titles': 'Marketing Manager|Business Development Manager'\n",
        "        })\n",
        "\n",
        "        apollo_template.to_csv('apollo_upload.csv', index=False)\n",
        "        print(\"Export r√©ussi! Fichier: apollo_upload.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur principale: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMgHt2vCfh9G",
        "outputId": "7479e44a-1091-448c-f484-887b18739730"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export r√©ussi! Fichier: apollo_upload.csv\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\"Write a Python script that scrapes product details, raw materials details (from the provided Excel file), and brand details from the website https://natrue.org/our-standard/natrue-certified-world/?database[tab]=products. Extract data for each product, including product name, description, certification status, and other available details. Also, extract raw materials and brand information. Store all details in a structured JSON format and then export this data into Google Sheets. Ensure the script handles pagination on the website.\"\n"
      ],
      "metadata": {
        "id": "SyJIiPOYbsL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztJFy44BasEQ",
        "outputId": "c0a9dda0-2030-49af-b4a2-01e48b42c102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.1.4)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.1)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.9.1->oauth2client) (3.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas gspread oauth2client\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Fonction pour récupérer les détails des produits\n",
        "def get_product_details():\n",
        "    url = \"https://natrue.org/our-standard/natrue-certified-world/?database[tab]=products\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extraire les informations des produits, ici un exemple\n",
        "    products = []\n",
        "    for product in soup.find_all('div', class_='product-class'):  # Exemple de classe à ajuster\n",
        "        name = product.find('h3').text\n",
        "        description = product.find('p', class_='product-description').text\n",
        "        certification = product.find('span', class_='certification-status').text\n",
        "        products.append({\"name\": name, \"description\": description, \"certification\": certification})\n",
        "\n",
        "    return products\n",
        "\n",
        "# Fonction pour envoyer les données vers Google Sheets\n",
        "def send_to_google_sheets(data, sheet_name=\"ProductData\"):\n",
        "    # Créer des credentials pour Google Sheets API\n",
        "    scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/drive']\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name('/content/Projet_stage.json', scope)\n",
        "    client = gspread.authorize(creds)\n",
        "\n",
        "    # Ouvrir la feuille de calcul\n",
        "    # Ouvrir la feuille de calcul\n",
        "    sheet = client.open('stage').sheet1  # Remarquez les guillemets autour de 'stage'\n",
        "\n",
        "    # Convertir en dataframe et envoyer\n",
        "    df = pd.DataFrame(data)\n",
        "    for i, row in df.iterrows():\n",
        "        sheet.append_row(row.values.tolist())\n",
        "\n",
        "# Récupérer les détails des produits et envoyer à Google Sheets\n",
        "products = get_product_details()\n",
        "send_to_google_sheets(products)\n",
        "\n",
        "# Sauvegarder en JSON\n",
        "with open('/content/sample_data/data.json', 'w') as json_file:\n",
        "    json.dump(products, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "fPMItdDwazPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temps estimé : 6–8 heures (scraping + tests + intégration)."
      ],
      "metadata": {
        "id": "ZFRiAgAzaNmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tâche 2 : Générer un prompt GPT pour extraire les matières premières de l'INCI et les structurer"
      ],
      "metadata": {
        "id": "yKoqM4NpXQGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python script that extracts individual raw materials from an INCI list and structures them similar to the material page at https://www.commonshare.com/materials/alpaca. The script should extract relevant details such as material name, description, properties, standards, and countries associated with each material. Ensure that the standards and countries are correctly matched with the materials. The final structure should be a well-organized data format, such as JSON, with each material having its own page for documentation.\n"
      ],
      "metadata": {
        "id": "mK6CBuEPXVuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 google-auth google-auth-oauthlib google-auth-httplib2 gspread\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxQj5StYx8l",
        "outputId": "86d68ef6-d5a0-44a3-b23f-a1ec931a4c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Act as a data engineer. Write a Python script to parse INCI strings (ex: 'Aqua, Glycerin, Cocos Nucifera Oil') into individual raw materials. For each material, create a Google Doc mimicking https://www.commonshare.com/materials/alpaca with:\n",
        "\n",
        "1. Standardized name (ISO 16128 ou INCI).\n",
        "2. CAS Number (via PubChem API).\n",
        "3. Sourcing Countries (géolocalisation via Wikidata).\n",
        "4. Certifications (COSMOS, Ecocert, etc.).\n",
        "5. Structure JSON : {name: 'Cocos Nucifera Oil', type: 'Plant', origin: ['Philippines', 'Brazil'], certifications: ['COSMOS']}.\n",
        "\n",
        "Validate standards and countries with official databases. Save outputs to Google Drive.\""
      ],
      "metadata": {
        "id": "tCjnstHdN22A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests gspread google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client pywikibot pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln6JTOH6O1ow",
        "outputId": "46e3cc02-ed7e-4fdb-d255-390f420469f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.1.4)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.160.0)\n",
            "Collecting pywikibot\n",
            "  Downloading pywikibot-10.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Collecting mwparserfromhell>=0.5.2 (from pywikibot)\n",
            "  Downloading mwparserfromhell-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pywikibot) (24.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.69.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Downloading pywikibot-10.0.0-py3-none-any.whl (718 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.5/718.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mwparserfromhell-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mwparserfromhell, pywikibot\n",
            "Successfully installed mwparserfromhell-0.6.6 pywikibot-10.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "def parse_inci(inci_string):\n",
        "    materials = inci_string.split(', ')\n",
        "    for material in materials:\n",
        "        # Recherche CAS via PubChem\n",
        "        cas = requests.get(f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{material}/property/InChIKey/TXT\")\n",
        "        # Géolocalisation via Wikidata...\n",
        "        # Création Google Doc\n",
        "        doc_body = {\n",
        "            'title': material,\n",
        "            'body': {\n",
        "                'content': [{\n",
        "                    'paragraph': {\n",
        "                        'elements': [{\n",
        "                            'textRun': {'content': f\"CAS: {cas}\\nOrigin: {countries}\"}\n",
        "                        }]\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        }\n",
        "        drive_service.documents().create(body=doc_body).execute()"
      ],
      "metadata": {
        "id": "rhM7O5U4NJr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_inci_string(inc_str):\n",
        "    materials = [material.strip() for material in inc_str.split(',')]\n",
        "    return materials\n"
      ],
      "metadata": {
        "id": "JeKAMRxLPMnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_cas_number(material):\n",
        "    url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{material}/property/CAS/JSON'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        try:\n",
        "            return data['PropertyTable']['Properties'][0]['CAS']\n",
        "        except (KeyError, IndexError):\n",
        "            return None\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "ZHiVF8pUPQjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYWIKIBOT_NO_USER_CONFIG\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "vpDjJPNpQTb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.http import MediaFileUpload\n"
      ],
      "metadata": {
        "id": "qnPBCEktU1qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "good :16HoOyG6PzmZcf4a8OoWXvhVjsp62n9Tr"
      ],
      "metadata": {
        "id": "vAT_L1sjZxs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from google.oauth2.service_account import Credentials\n",
        "import time\n",
        "\n",
        "# 1. Configuration des scopes et credentials\n",
        "SCOPES = [\n",
        "    \"https://www.googleapis.com/auth/drive\",\n",
        "    \"https://www.googleapis.com/auth/documents\"\n",
        "]\n",
        "\n",
        "def get_google_credentials(credential_path):\n",
        "    return Credentials.from_service_account_file(credential_path, scopes=SCOPES)\n",
        "\n",
        "# 2. Parsing de la chaîne INCI\n",
        "def parse_inci_string(inci_str):\n",
        "    return [material.strip() for material in inci_str.split(',')]\n",
        "\n",
        "# 3. Récupération du numéro CAS depuis PubChem (avec gestion d'erreur)\n",
        "def get_cas_number(material):\n",
        "    url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{material}/property/CAS/JSON'\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['PropertyTable']['Properties'][0]['CAS']\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur CAS pour {material}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# 4. Géolocalisation depuis Wikidata (requête SPARQL améliorée)\n",
        "def get_geolocation_from_wikidata(material):\n",
        "    query = f\"\"\"\n",
        "    SELECT ?countryLabel WHERE {{\n",
        "        ?item rdfs:label \"{material}\"@en.\n",
        "        ?item wdt:P17 ?country.\n",
        "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            \"https://query.wikidata.org/sparql\",\n",
        "            params={'query': query, 'format': 'json'},\n",
        "            headers={'User-Agent': 'Mozilla/5.0'},\n",
        "            timeout=15\n",
        "        )\n",
        "        return list(set([result['countryLabel']['value'] for result in response.json()['results']['bindings']]))\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur Wikidata pour {material}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# 5. Création du document Google dans votre dossier personnel\n",
        "def create_google_doc(drive_service, docs_service, material_data, folder_id):\n",
        "    try:\n",
        "        # Création du document dans le dossier spécifié\n",
        "        file_metadata = {\n",
        "            'name': f\"Fiche - {material_data['name']}\",\n",
        "            'parents': [folder_id],\n",
        "            'mimeType': 'application/vnd.google-apps.document'\n",
        "        }\n",
        "        doc = drive_service.files().create(body=file_metadata).execute()\n",
        "        doc_id = doc['id']\n",
        "\n",
        "        # Ajout du contenu\n",
        "        requests = [{\n",
        "            'insertText': {\n",
        "                'location': {'index': 1},\n",
        "                'text': (\n",
        "                    f\"Nom: {material_data['name']}\\n\"\n",
        "                    f\"CAS: {material_data['cas']}\\n\"\n",
        "                    f\"Origines: {', '.join(material_data['origins'])}\\n\"\n",
        "                    f\"Certifications: {', '.join(material_data['certifications'])}\\n\"\n",
        "                )\n",
        "            }\n",
        "        }]\n",
        "        docs_service.documents().batchUpdate(\n",
        "            documentId=doc_id,\n",
        "            body={'requests': requests}\n",
        "        ).execute()\n",
        "\n",
        "        print(f\"✅ Document créé: https://docs.google.com/document/d/{doc_id}\")\n",
        "        return doc_id\n",
        "    except HttpError as error:\n",
        "        print(f\"❌ Erreur Google API: {error}\")\n",
        "        return None\n",
        "\n",
        "# 6. Processus principal\n",
        "def process_inci(inci_str, credentials_path, folder_id):\n",
        "    credentials = get_google_credentials(credentials_path)\n",
        "    drive_service = build('drive', 'v3', credentials=credentials)\n",
        "    docs_service = build('docs', 'v1', credentials=credentials)\n",
        "\n",
        "    for material in parse_inci_string(inci_str):\n",
        "        print(f\"\\n🔍 Traitement de: {material}\")\n",
        "\n",
        "        doc_id = create_google_doc(\n",
        "            drive_service,\n",
        "            docs_service,\n",
        "            {\n",
        "                'name': material,\n",
        "                'cas': get_cas_number(material) or \"Non trouvé\",\n",
        "                'origins': get_geolocation_from_wikidata(material),\n",
        "                'certifications': ['COSMOS']  # À adapter avec votre logique\n",
        "            },\n",
        "            folder_id\n",
        "        )\n",
        "        time.sleep(1)  # Respect des quotas d'API\n",
        "\n",
        "# 7. Exécution\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration requise\n",
        "    INCI_EXAMPLE = \"Aqua, Glycerin, Cocos Nucifera Oil\"\n",
        "    CREDENTIALS_PATH = \"/content/projetstage1.json\"  # À modifier\n",
        "    FOLDER_ID = \"16HoOyG6PzmZcf4a8OoWXvhVjsp62n9Tr\"  # À modifier\n",
        "\n",
        "    process_inci(INCI_EXAMPLE, CREDENTIALS_PATH, FOLDER_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BIj0RJjUlS7",
        "outputId": "be545c54-9324-4331-9666-0bf867dc69f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Traitement de: Aqua\n",
            "✅ Document créé: https://docs.google.com/document/d/1JzOzLtS7S4ooaKqsXgy1suWcBBk_OzaaIe4LwJttjeo\n",
            "\n",
            "🔍 Traitement de: Glycerin\n",
            "✅ Document créé: https://docs.google.com/document/d/14bz5jkp-hhozQ8ILB1UgPD-ujiIJ5nz_n4nMJSqBTAA\n",
            "\n",
            "🔍 Traitement de: Cocos Nucifera Oil\n",
            "✅ Document créé: https://docs.google.com/document/d/1Z7cJvSlgMKEnVxTDKnqrW1lQfPi90BdjFqyPoOyohng\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3 (Extra Credit) : Trouver les Contacts Clés"
      ],
      "metadata": {
        "id": "fJQk-Vdte7AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outils Low-Code :\n",
        "\n",
        "\n",
        "\n",
        "1.   Apollo.io ou Hunter.io : Pour trouver des emails et postes.\n",
        "2.   LinkedIn Sales Navigator : Pour identifier les profils.\n",
        "3.   Zapier : Automatiser la sauvegarde dans Google Sheets.\n",
        "4.   Phantombuster : Scraper LinkedIn automatiquement.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7KQsuaVKfHv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.oauth2.service_account import Credentials\n",
        "import gspread\n",
        "import re\n",
        "\n",
        "# 1. Connexion à Google Sheets (version corrigée)\n",
        "def connect_google_sheets(credentials_path, sheet_id):\n",
        "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
        "\n",
        "    try:\n",
        "        creds = Credentials.from_service_account_file(credentials_path, scopes=SCOPES)\n",
        "        client = gspread.authorize(creds)\n",
        "        return client.open_by_key(sheet_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de connexion: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# 2. Extraction du nom de marque (version améliorée)\n",
        "def extract_brand(name):\n",
        "    try:\n",
        "        # Cas 1 : Séparateur \"T/A\"\n",
        "        if \" T/A \" in name:\n",
        "            return name.split(\" T/A \")[-1].split(\"(\")[0].strip()\n",
        "\n",
        "        # Cas 2 : Marque entre crochets ou avec ™\n",
        "        brand_match = re.search(r'(\\[.*?\\]™)|\\((.*?)\\)', name)\n",
        "        if brand_match:\n",
        "            return brand_match.group(1) or brand_match.group(2)\n",
        "\n",
        "        # Cas 3 : Détection automatique du fabricant\n",
        "        patterns = [\n",
        "            r'(.*?)\\s+(GmbH|SAS|AG|Ltd|INC|LLC)',\n",
        "            r'(.*?)\\s+\\d',\n",
        "            r'^(.*?)\\s+-\\s+'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.match(pattern, name)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "\n",
        "        return name.split()[0]\n",
        "    except:\n",
        "        return \"N/A\"\n",
        "\n",
        "# 3. Nettoyage des noms (version sécurisée)\n",
        "def clean_company_names(df):\n",
        "    try:\n",
        "        df = df.copy()\n",
        "        df['cleaned_name'] = df['Brand Name'].str.strip().str.upper()\n",
        "\n",
        "        # Suppression des entités légales et caractères spéciaux\n",
        "        df['cleaned_name'] = df['cleaned_name'].str.replace(\n",
        "            r'\\b(INC|LLC|LTD|SA|SAS|GMBH|PTY|CORP|PLC)\\b[,.]?$',\n",
        "            '',\n",
        "            regex=True\n",
        "        ).str.strip()\n",
        "\n",
        "        df['cleaned_name'] = df['cleaned_name'].apply(\n",
        "            lambda x: re.sub(r'[^a-zA-Z0-9À-ÿ& ]', '', x) if isinstance(x, str) else x\n",
        "        )\n",
        "\n",
        "        return df.drop_duplicates('cleaned_name').dropna(subset=['cleaned_name'])\n",
        "    except KeyError:\n",
        "        print(\"Colonne 'Brand Name' manquante!\")\n",
        "        return df\n",
        "\n",
        "# 4. Workflow principal (version corrigée)\n",
        "def main():\n",
        "    # Configuration\n",
        "    CREDS_PATH = '/content/projetstage1.json'\n",
        "    SHEET_ID = '1gPypXrLKXphNi03Y9cl9njUJ7XEPFU6jaIETt7O4NdA'\n",
        "    SHEET_NAME = 'Worksheet'\n",
        "\n",
        "    try:\n",
        "        # Connexion\n",
        "        sheet = connect_google_sheets(CREDS_PATH, SHEET_ID)\n",
        "        if not sheet:\n",
        "            return\n",
        "\n",
        "        worksheet = sheet.worksheet(SHEET_NAME)\n",
        "        data = worksheet.get_all_values()\n",
        "\n",
        "        # Création du DataFrame\n",
        "        df = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "        # Extraction des marques\n",
        "        df[\"Brand Name\"] = df[\"Name\"].apply(extract_brand)\n",
        "\n",
        "        # Nettoyage\n",
        "        cleaned_df = clean_company_names(df)\n",
        "\n",
        "        # Export Apollo\n",
        "        apollo_template = pd.DataFrame({\n",
        "            'Company Name': cleaned_df['cleaned_name'],\n",
        "            'Target Job Titles': 'Marketing Manager|Business Development Manager'\n",
        "        })\n",
        "\n",
        "        apollo_template.to_csv('apollo_upload.csv', index=False)\n",
        "        print(\"Export réussi! Fichier: apollo_upload.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur principale: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMgHt2vCfh9G",
        "outputId": "7479e44a-1091-448c-f484-887b18739730"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export réussi! Fichier: apollo_upload.csv\n"
          ]
        }
      ]
    }
  ]
}